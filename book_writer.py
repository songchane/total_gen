# book_writer.py â€” BI ìƒê¶Œ íŠ¸ë Œë“œ ë¶„ì„ ì—”ì§„


from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import (
    AnyMessage,
    SystemMessage,
    HumanMessage,
    AIMessage,
)
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers.string import StrOutputParser
from typing_extensions import TypedDict
from typing import List, Dict, Any, Optional
from datetime import datetime
import json

from dotenv import load_dotenv

from tools import retrieve, web_search, add_web_pages_json_to_chroma

load_dotenv()


# ëª¨ë¸ & ìƒíƒœ ì •ì˜

llm = ChatOpenAI(model="gpt-4o")


class State(TypedDict):
    messages: List[AnyMessage]
    references: Dict[str, Any]  # {"queries": [...], "docs": [...]}
    user_request: Dict[str, Any]  # {"region": ..., "industry": ...}
    report: Optional[str]


# 1. ì¿¼ë¦¬ íŒŒì„œ (Business Analyst)

def parse_query(state: State) -> State:
    print("\n\n============ PARSE QUERY (Business Analyst) ============")

    messages = state["messages"]
    user_last = ""
    for m in reversed(messages):
        if isinstance(m, HumanMessage):
            user_last = m.content
            break

    prompt = PromptTemplate.from_template(
        """
        ë„ˆëŠ” 'ì •ì„± ê¸°ë°˜ ìƒê¶Œ BI ë¶„ì„ ì±—ë´‡'ì˜ ì¿¼ë¦¬ íŒŒì„œì´ë‹¤.

        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ JSON í˜•ì‹ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ë¼.

        ```json
        {{
          "region": "ì˜ˆ: ì„±ìˆ˜ë™, ìš©ì‚°êµ¬, í™ëŒ€ì…êµ¬, ë¶€ì‚° ì„œë©´",
          "industry": "ì˜ˆ: ì¹´í˜, F&B, ì™¸ì‹ì—…, íŒ¨ì…˜, ì „ì²´",
          "period": "ì˜ˆ: ìµœê·¼ 6ê°œì›”, ìµœê·¼ 1ë…„, 2024ë…„, 2023ë…„ 1~6ì›”",
          "keywords": ["íŠ¸ë Œë“œ", "í•«í”Œ", "ì†Œë¹„ì íŒ¨í„´", "ì—…ì¢… ë³€í™”"]
        }}
        ```

        - regionì€ ìƒê¶Œ ë¶„ì„ì˜ ì¤‘ì‹¬ì´ ë˜ëŠ” ì£¼ìš” ì§€ì—­ëª…ìœ¼ë¡œ ì±„ì›Œë¼.
        - industryëŠ” ì§ˆë¬¸ì— ëª…ì‹œëœ ì—…ì¢…(ì¹´í˜, ìŒì‹ì  ë“±)ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ, ì—†ìœ¼ë©´ 'ì „ì²´'ë¡œ ì±„ì›Œë¼.
        - periodëŠ” ì§ˆë¬¸ì— 'ìµœê·¼ 1ë…„', 'ìµœê·¼ 6ê°œì›”' ë“±ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ, ì—†ìœ¼ë©´ 'ìµœê·¼ 1ë…„'ìœ¼ë¡œ ì±„ì›Œë¼.
        - keywordsëŠ” ìµœì†Œ ["íŠ¸ë Œë“œ"] í•œ ê°œëŠ” ë°˜ë“œì‹œ í¬í•¨í•˜ë¼.

        ì‚¬ìš©ì ì§ˆë¬¸: {user_last_comment}
        """
    )

    chain = prompt | llm | StrOutputParser()
    raw_json = chain.invoke({"user_last_comment": user_last})

    # ```json ... ``` ì œê±°
    cleaned = raw_json.replace("```json", "").replace("```", "").strip()
    try:
        parsed = json.loads(cleaned)
    except Exception:
        parsed = {
            "region": user_last,
            "industry": "ì „ì²´",
            "period": "ìµœê·¼ 1ë…„",
            "keywords": ["íŠ¸ë Œë“œ"],
        }

    print(f"[Parsed Query] {parsed}")

    state["user_request"] = parsed
    state["messages"].append(AIMessage(content=f"[Parsed] {parsed}"))
    return state


# 2. Web Search (DuckDuckGo + JSON + Chroma)

def web_search_node(state: State) -> State:
    print("\n\n============ WEB SEARCH ============")

    req = state["user_request"]
    region = req.get("region", "")
    industry = req.get("industry", "")
    period = req.get("period", "ìµœê·¼ 1ë…„")
    keywords = req.get("keywords", ["íŠ¸ë Œë“œ"])

    # LLM ì—†ì´ Pythonì—ì„œ ì•ˆì „í•˜ê²Œ ê²€ìƒ‰ì–´ êµ¬ì„±
    base = f"{region} {industry}".strip()
    if not base:
        base = region or industry or "ìƒê¶Œ"

    queries = [
        f"{base} ìƒê¶Œ íŠ¸ë Œë“œ {period}",
        f"{base} ì¹´í˜ íŠ¸ë Œë“œ {period}" if industry == "ì¹´í˜" else f"{base} ì†Œë¹„ì íŒ¨í„´ {period}",
        f"{base} í•«í”Œ ìƒê¶Œ ë³€í™” {period}",
    ]

    refs = state["references"]

    for q in queries:
        q = q.strip()
        if not q:
            continue
        print(f"[WebSearch] query={q}")
        try:
            # tools.web_search: (results, json_path) ê°€ì •
            _, json_path = web_search.invoke({"query": q})
            add_web_pages_json_to_chroma(json_path)
            refs["queries"].append(q)
        except Exception as e:
            print(f"[WebSearch Error] {e}")

    return state


# 3. Vector Search (RAG)

def vector_search_node(state: State) -> State:
    print("\n\n============ VECTOR SEARCH (RAG) ============")

    req = state["user_request"]
    region = req.get("region", "")
    industry = req.get("industry", "")
    period = req.get("period", "ìµœê·¼ 1ë…„")

    refs = state["references"]

    rag_queries = [
        f"{region} {industry} ìƒê¶Œ íŠ¸ë Œë“œ {period}",
        f"{region} {industry} ì†Œë¹„ì í–‰ë™ íŒ¨í„´ {period}",
        f"{region} {industry} ì¸ê¸° ì—…ì¢… ë³€í™” {period}",
    ]

    for q in rag_queries:
        q = q.strip()
        if not q:
            continue
        print(f"[RAG] query={q}")
        try:
            docs = retrieve.invoke({"query": q, "top_k": 5})
            refs["queries"].append(q)
            refs["docs"].extend(docs)
        except Exception as e:
            print(f"[RAG Error] {e}")

    # ì¤‘ë³µ ì œê±°
    seen = set()
    unique_docs = []
    for d in refs["docs"]:
        content = getattr(d, "page_content", "")
        if content not in seen:
            seen.add(content)
            unique_docs.append(d)
    refs["docs"] = unique_docs

    print(f"[RAG] ì´ ë¬¸ì„œ ìˆ˜: {len(refs['docs'])}")
    return state


# 4. Content Strategist (BI ë³´ê³ ì„œ ìƒì„±)

def content_strategist_node(state: State) -> State:
    print("\n\n============ CONTENT STRATEGIST ============")

    req = state["user_request"]
    refs = state["references"]

    region = req.get("region", "í•´ë‹¹ ì§€ì—­")
    industry = req.get("industry", "ì „ì²´")
    period = req.get("period", "ìµœê·¼ 1ë…„")

    # RAG ë¬¸ì„œ ìš”ì•½ ì¼ë¶€ ì¶”ì¶œ
    doc_snippets = []
    for d in refs.get("docs", [])[:10]:
        text = getattr(d, "page_content", "")
        snippet = text[:500].replace("\n", " ")
        if snippet:
            doc_snippets.append(snippet)
    docs_text = "\n\n---\n\n".join(doc_snippets) if doc_snippets else "ê´€ë ¨ ë¬¸ì„œê°€ ê±°ì˜ ì—†ìŒ"

    prompt = PromptTemplate.from_template(
        """
        ë„ˆëŠ” 'ìƒê¶Œ BI íŠ¸ë Œë“œ ë¶„ì„' ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ë‹¤.

        ì•„ë˜ region, industry, period, ê·¸ë¦¬ê³  ì°¸ê³  ë¬¸ì„œ(RAG)ë¥¼ ë°”íƒ•ìœ¼ë¡œ
        ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì¸ì‚¬ì´íŠ¸ ì¤‘ì‹¬ì˜ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ë¼.

        ë°˜ë“œì‹œ ì•„ë˜ ëª©ì°¨ë¥¼ ë”°ë¥´ë˜, ë‚´ìš©ì€ êµ¬ì²´ì ì´ê³  ì „ëµì  ì¸ì‚¬ì´íŠ¸ ìœ„ì£¼ë¡œ ì±„ì›Œë¼.

        # {region} ìƒê¶Œ íŠ¸ë Œë“œ ë¶„ì„ ë³´ê³ ì„œ

        ## 1. ê°œìš”
        - ë¶„ì„ ì§€ì—­: {region}
        - ì—…ì¢… ë²”ìœ„: {industry}
        - ë¶„ì„ ê¸°ê°„: {period}
        - ë°ì´í„° ì¶œì²˜: DuckDuckGo Web RAG + GPT ë¶„ì„
        - ë¶„ì„ ëª©ì : ìƒê¶Œ íŠ¸ë Œë“œ íŒŒì•… ë° ìœ ë§ ì—…ì¢… ì „ëµ ìˆ˜ë¦½ ì§€ì›

        ## 2. ìƒê¶Œ í•µì‹¬ ìš”ì•½(Key Summary)
        - í•´ë‹¹ ìƒê¶Œì˜ í•µì‹¬ íŠ¸ë Œë“œ Top 3~5
        - ì†Œë¹„ì í–‰ë™ ë° ë°©ë¬¸ íŒ¨í„´ ìš”ì•½
        - ì¸ê¸° ì—…ì¢…/ì½˜ì…‰íŠ¸ ìš”ì•½
        - ìƒê¶Œì˜ ê°•ì ê³¼ ì•½ì  í•œ ì¤„ì”© ì •ë¦¬

        ## 3. í™˜ê²½ ë³€í™” ë¶„ì„(Macro & Local Trend)
        ### 3.1 ì™¸ë¶€ í™˜ê²½ ë° ê°œë°œ ì´ìŠˆ
        - êµí†µ ì¸í”„ë¼, ì¬ê°œë°œ/ë„ì‹œê³„íš, ìƒê¶Œ í™•ì¥/ì¶•ì†Œ ê´€ë ¨ ì´ìŠˆ
        - ì •ì±…, ê·œì œ, ìƒì—…ì§€ì—­ ì¡°ì • ë“± ìƒê¶Œì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì†Œ

        ### 3.2 ì†Œë¹„ì í–‰ë™ íŠ¸ë Œë“œ
        - ì£¼ ê³ ê°ì¸µ(ì—°ë ¹/ì§ì—…/ë¼ì´í”„ìŠ¤íƒ€ì¼)ì˜ íŠ¹ì§•
        - ì†Œë¹„ ì„±í–¥(ê°€ì‹¬ë¹„/ê°€ì„±ë¹„/SNS ì¸ì¦/ê²½í—˜ ì¤‘ì‹¬ ë“±)
        - ìš”ì¼/ì‹œê°„ëŒ€ë³„ ë°©ë¬¸ íŒ¨í„´ ë³€í™”

        ## 4. ìƒê¶Œ êµ¬ì¡° ë° ê²½ìŸ ë¶„ì„
        ### 4.1 í•µì‹¬ ìƒê¶Œ êµ¬ì—­ 2~4ê³³ ë¶„ì„
        - ê° êµ¬ì—­ì˜ ìƒê¶Œ ì„±ê²©(ê´€ê´‘/ë¡œì»¬/ì˜¤í”¼ìŠ¤/ì£¼ê±° ë“±)
        - ì—…ì¢… êµ¬ì„± ë¹„ìœ¨ê³¼ íŠ¹ì§•
        - ì‹ ê·œ ë¸Œëœë“œ/íŒì—…/íì  ë“± ìµœê·¼ ë³€í™”

        ### 4.2 ê²½ìŸë„Â·í¬í™”ë„ ë¶„ì„
        - ì—…ì¢…ë³„ ê²½ìŸ ê°•ë„ (ì¹´í˜/ë””ì €íŠ¸/F&B/íŒ¨ì…˜ ë“±)
        - ê³¼ë°€/ê³¼ì†Œ êµ¬ì—­ì— ëŒ€í•œ ì •ì„±ì  íŒë‹¨
        - ì§„ì… ì¥ë²½ ë° ì°¨ë³„í™” í¬ì¸íŠ¸

        ## 5. ì—…ì¢…ë³„ íŠ¸ë Œë“œ ì„¸ë¶€ ë¶„ì„
        ### 5.1 ì¹´í˜ & ë””ì €íŠ¸
        - ì¸ê¸° ë©”ë‰´, ì¸í…Œë¦¬ì–´/ì½˜ì…‰íŠ¸, ì²´ë¥˜ ì‹œê°„ ë“±
        - ì†Œë¹„ì ë‹ˆì¦ˆì™€ ë°©ë¬¸ ë™ê¸°

        ### 5.2 ë ˆìŠ¤í† ë‘ & F&B (í•´ë‹¹ ì‹œ)
        - ì£¼ëª©ë°›ëŠ” ìŒì‹ ì¹´í…Œê³ ë¦¬
        - ë°ì´íŠ¸/ëª¨ì„/ì¼ìƒ ì†Œë¹„ ë“± ë°©ë¬¸ ëª©ì 

        ### 5.3 íŒ¨ì…˜/ë¼ì´í”„ìŠ¤íƒ€ì¼ (í•´ë‹¹ ì‹œ)
        - ë¡œì»¬ ë¸Œëœë“œ, íŒì—… ìŠ¤í† ì–´, ë¼ì´í”„ìŠ¤íƒ€ì¼ ë³€í™”

        ## 6. ê¸°íšŒ ìš”ì¸(Strength / Opportunity)
        - ì„±ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì—…ì¢…/ì½˜ì…‰íŠ¸
        - ë¹ ë¥´ê²Œ ì¦ê°€í•˜ëŠ” ì†Œë¹„ì ìœ í˜•
        - ê²½ìŸì´ ìƒëŒ€ì ìœ¼ë¡œ ëœí•œ ë‹ˆì¹˜ ì˜ì—­

        ## 7. ë¦¬ìŠ¤í¬ ìš”ì¸(Weakness / Threat)
        - ê²½ìŸ ê³¼ì—´ ì—…ì¢…
        - ìœ í–‰ í”¼ë¡œë„ ë° ë‹¨ê¸° íŠ¸ë Œë“œ ìœ„í—˜
        - ë¹„ìš©Â·ì„ëŒ€ë£ŒÂ·ì…ì§€ ê´€ë ¨ ë¦¬ìŠ¤í¬

        ## 8. GPT ì¶”ì²œ ì—…ì¢… Top 3 & ì¶”ì²œ ì´ìœ 
        - í–¥í›„ 6~12ê°œì›” ê¸°ì¤€ {region} ìƒê¶Œì—ì„œ ìœ ë§í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ì—…ì¢… 3ê°œë¥¼ ì„ ì •í•˜ë¼.
        - ê° ì—…ì¢…ë§ˆë‹¤:
          - ì¶”ì²œ ì—…ì¢…ëª…
          - ì¶”ì²œ ì´ìœ  (ìˆ˜ìš”, ê²½ìŸë„, íŠ¸ë Œë“œ, ì†Œë¹„ì íŠ¹ì„± ê·¼ê±°)
          - ì‹¤ì œë¡œ ì–´ë–¤ ì½˜ì…‰íŠ¸ë¡œ í’€ë©´ ì¢‹ì„ì§€ ê°„ë‹¨ ì œì•ˆ

        ## 9. ì¢…í•© ê²°ë¡ 
        - ìƒê¶Œì˜ ì¤‘ì¥ê¸° ë°©í–¥ì„± ìš”ì•½
        - ì‚¬ì—…ì/ë¸Œëœë“œ/ì°½ì—…ìì—ê²Œ ì œì‹œí•˜ëŠ” í•œ ì¤„ ì „ëµ ì •ë¦¬

        -----------------------------------------
        [ì°¸ê³ ìš© RAG ë¬¸ì„œ ìš”ì•½]
        {docs_text}
        -----------------------------------------

        ìœ„ êµ¬ì¡°ì— ë”°ë¼ í•˜ë‚˜ì˜ Markdown ë¬¸ì„œë¡œë§Œ ë‹µë³€í•˜ë¼.
        """
    )

    chain = prompt | llm | StrOutputParser()
    report = chain.invoke(
        {
            "region": region,
            "industry": industry,
            "period": period,
            "docs_text": docs_text,
        }
    )

    print("\n\n===== ìƒì„±ëœ ìƒê¶Œ BI íŠ¸ë Œë“œ ë¶„ì„ ë³´ê³ ì„œ =====\n")
    print(report[:500] + "\n... (ì¤‘ëµ) ...\n")  # ì½˜ì†”ì—ëŠ” ì•ë¶€ë¶„ë§Œ

    state["report"] = report
    state["messages"].append(AIMessage(content=report))
    return state


# 5. LangGraph êµ¬ì„±

graph_builder = StateGraph(State)

graph_builder.add_node("parse_query", parse_query)
graph_builder.add_node("web_search", web_search_node)
graph_builder.add_node("vector_search", vector_search_node)
graph_builder.add_node("content_strategist", content_strategist_node)

graph_builder.add_edge(START, "parse_query")
graph_builder.add_edge("parse_query", "web_search")
graph_builder.add_edge("web_search", "vector_search")
graph_builder.add_edge("vector_search", "content_strategist")
graph_builder.add_edge("content_strategist", END)

graph = graph_builder.compile()


# 6. ì½˜ì†” í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ ë£¨í”„ (ì„ íƒì‚¬í•­)

if __name__ == "__main__":
    print("ğŸš€ ìƒê¶Œ BI íŠ¸ë Œë“œ ë¶„ì„ ì½˜ì†” ë²„ì „ì…ë‹ˆë‹¤. ì¢…ë£Œ: exit / q / quit\n")
    while True:
        user_input = input("\nUser    : ").strip()
        if user_input.lower() in ("exit", "quit", "q"):
            print("Goodbye!")
            break

        init_state: State = {
            "messages": [
                SystemMessage(
                    f"ë„ˆëŠ” ìƒê¶Œ BI íŠ¸ë Œë“œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë°ì´í„° ë¶„ì„ íŒ€ì´ë‹¤. "
                    f"í˜„ì¬ ì‹œê°ì€ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ì´ë‹¤."
                ),
                HumanMessage(user_input),
            ],
            "references": {"queries": [], "docs": []},
            "user_request": {},
            "report": None,
        }

        final_state = graph.invoke(init_state)
        report = final_state.get("report")

        if report:
            print("\n\n====== ìµœì¢… ë³´ê³ ì„œ ======\n")
            print(report)
        else:
            print("âŒ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


# 7. Streamlit ë“±ì—ì„œ í˜¸ì¶œí•  í•¨ìˆ˜

def run_analysis(user_input: str) -> str:
    """ì™¸ë¶€(UI)ì—ì„œ í˜¸ì¶œí•˜ëŠ” ë‹¨ì¼ ë¶„ì„ í•¨ìˆ˜."""
    init_state: State = {
        "messages": [
            SystemMessage(
                f"ë„ˆëŠ” ìƒê¶Œ BI íŠ¸ë Œë“œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë°ì´í„° ë¶„ì„ íŒ€ì´ë‹¤. "
                f"í˜„ì¬ ì‹œê°ì€ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ì´ë‹¤."
            ),
            HumanMessage(user_input),
        ],
        "references": {"queries": [], "docs": []},
        "user_request": {},
        "report": None,
    }

    final_state = graph.invoke(init_state)
    report = final_state.get("report")

    if report:
        return report

    # í˜¹ì‹œ report í‚¤ê°€ ë¹„ì–´ìˆìœ¼ë©´ messagesì—ì„œ ë‹¤ì‹œ ì°¾ì•„ë³´ê¸°
    for m in final_state["messages"]:
        if isinstance(m, AIMessage):
            txt = m.content or ""
            if txt.strip().startswith("# "):
                return txt

    return "âŒ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë³´ê³ ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”)"
